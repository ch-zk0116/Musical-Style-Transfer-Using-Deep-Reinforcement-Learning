{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cab1ba06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model classes imported successfully\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_1824\\3915151438.py:165: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(self.config.RAGAN_MODEL_PATH, map_location=self.config.DEVICE))\n"
     ]
    }
   ],
   "source": [
    "# MIDI Style Transfer UI - Jupyter Notebook Cell\n",
    "# Run this cell to launch the interactive UI\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "import pretty_midi\n",
    "from fluidsynth import Synth\n",
    "import wave\n",
    "import tkinter as tk\n",
    "from tkinter import filedialog, messagebox, ttk\n",
    "import threading\n",
    "from pathlib import Path\n",
    "import tempfile\n",
    "import shutil\n",
    "\n",
    "# Import your model classes (make sure these are available)\n",
    "try:\n",
    "    from GAN import Generator\n",
    "    from TransformerVAE import TransformerVAE\n",
    "    print(\"Model classes imported successfully\")\n",
    "except ImportError as e:\n",
    "    print(f\"Warning: Could not import model classes - {e}\")\n",
    "    print(\"Make sure GAN.py and TransformerVAE.py are in your Python path\")\n",
    "\n",
    "class MidiStyleTransferApp:\n",
    "    def __init__(self):\n",
    "        # Configuration\n",
    "        self.config = self.create_config()\n",
    "        self.models = {}\n",
    "        self.selected_model = None\n",
    "        self.input_midi_path = None\n",
    "        \n",
    "        # Create GUI\n",
    "        self.root = tk.Tk()\n",
    "        self.root.title(\"MIDI Style Transfer\")\n",
    "        self.root.geometry(\"600x500\")\n",
    "        \n",
    "        self.setup_gui()\n",
    "        \n",
    "    def create_config(self):\n",
    "        \"\"\"Create configuration object with default paths - update these for your system\"\"\"\n",
    "        class Config:\n",
    "            # Update these paths for your system\n",
    "            SOUNDFONT_PATH = r\"C:\\Users\\User\\Desktop\\college\\fyp\\other\\soundfont\\GeneralUser GS v1.471.sf2\"\n",
    "            OUTPUT_DIR = r\"C:\\Users\\User\\Desktop\\college\\fyp\\converted_samples\"\n",
    "            \n",
    "            # Model paths - update these\n",
    "            GAN_MODEL_PATH = r\"C:\\Users\\User\\Desktop\\college\\fyp\\models\\PPO_Tuned_GAN\\ppo_tuned_GAN_best.pth\"\n",
    "            RAGAN_MODEL_PATH = r\"C:\\Users\\User\\Desktop\\college\\fyp\\models\\PPO_Tuned_RaGAN\\ppo_tuned_RaGAN_best.pth\"\n",
    "            VAE_MODEL_PATH = r\"C:\\Users\\User\\Desktop\\college\\fyp\\models\\PPO_Tuned_VAE\\ppo_tuned_VAE_epoch_50.pth\"\n",
    "            \n",
    "            # Model parameters\n",
    "            DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "            PITCHES = 88\n",
    "            TIMESTEPS = 128\n",
    "            CHANNELS = 3\n",
    "            NOISE_DIM = 100\n",
    "            \n",
    "            # VAE parameters\n",
    "            LATENT_DIM = 256\n",
    "            EMBED_DIM = 512\n",
    "            NHEAD = 8\n",
    "            NUM_ENCODER_LAYERS = 6\n",
    "            NUM_DECODER_LAYERS = 6\n",
    "            DROPOUT = 0.1\n",
    "            \n",
    "        return Config()\n",
    "    \n",
    "    def setup_gui(self):\n",
    "        \"\"\"Setup the GUI components\"\"\"\n",
    "        # Main frame\n",
    "        main_frame = ttk.Frame(self.root, padding=\"10\")\n",
    "        main_frame.grid(row=0, column=0, sticky=(tk.W, tk.E, tk.N, tk.S))\n",
    "        \n",
    "        # Title\n",
    "        title_label = ttk.Label(main_frame, text=\"MIDI Style Transfer\", font=(\"Arial\", 16, \"bold\"))\n",
    "        title_label.grid(row=0, column=0, columnspan=2, pady=(0, 20))\n",
    "        \n",
    "        # Model selection\n",
    "        ttk.Label(main_frame, text=\"Select Model:\", font=(\"Arial\", 12)).grid(row=1, column=0, sticky=tk.W, pady=5)\n",
    "        self.model_var = tk.StringVar(value=\"Select a model...\")\n",
    "        model_dropdown = ttk.Combobox(main_frame, textvariable=self.model_var, \n",
    "                                    values=[\"GAN\", \"RaGAN\", \"VAE\"], state=\"readonly\", width=30)\n",
    "        model_dropdown.grid(row=1, column=1, sticky=(tk.W, tk.E), pady=5)\n",
    "        model_dropdown.bind(\"<<ComboboxSelected>>\", self.on_model_selected)\n",
    "        \n",
    "        # Load model button\n",
    "        self.load_model_btn = ttk.Button(main_frame, text=\"Load Model\", \n",
    "                                       command=self.load_model, state=tk.DISABLED)\n",
    "        self.load_model_btn.grid(row=2, column=1, sticky=tk.W, pady=5)\n",
    "        \n",
    "        # Model status\n",
    "        self.model_status_label = ttk.Label(main_frame, text=\"No model loaded\", foreground=\"red\")\n",
    "        self.model_status_label.grid(row=3, column=0, columnspan=2, pady=5)\n",
    "        \n",
    "        # File selection\n",
    "        ttk.Label(main_frame, text=\"Select MIDI File:\", font=(\"Arial\", 12)).grid(row=4, column=0, sticky=tk.W, pady=(20, 5))\n",
    "        self.file_btn = ttk.Button(main_frame, text=\"Browse MIDI File\", \n",
    "                                 command=self.browse_midi_file)\n",
    "        self.file_btn.grid(row=4, column=1, sticky=tk.W, pady=(20, 5))\n",
    "        \n",
    "        # Selected file display\n",
    "        self.file_label = ttk.Label(main_frame, text=\"No file selected\", foreground=\"gray\")\n",
    "        self.file_label.grid(row=5, column=0, columnspan=2, pady=5)\n",
    "        \n",
    "        # Process button\n",
    "        self.process_btn = ttk.Button(main_frame, text=\"Generate Style Transfer\", \n",
    "                                    command=self.process_midi, state=tk.DISABLED,\n",
    "                                    style=\"Accent.TButton\")\n",
    "        self.process_btn.grid(row=6, column=0, columnspan=2, pady=20)\n",
    "        \n",
    "        # Progress bar\n",
    "        self.progress = ttk.Progressbar(main_frame, mode='indeterminate')\n",
    "        self.progress.grid(row=7, column=0, columnspan=2, sticky=(tk.W, tk.E), pady=5)\n",
    "        \n",
    "        # Status text\n",
    "        self.status_text = tk.Text(main_frame, height=10, width=70, state=tk.DISABLED)\n",
    "        self.status_text.grid(row=8, column=0, columnspan=2, pady=10, sticky=(tk.W, tk.E, tk.N, tk.S))\n",
    "        \n",
    "        # Scrollbar for status text\n",
    "        scrollbar = ttk.Scrollbar(main_frame, orient=tk.VERTICAL, command=self.status_text.yview)\n",
    "        scrollbar.grid(row=8, column=2, sticky=(tk.N, tk.S), pady=10)\n",
    "        self.status_text.configure(yscrollcommand=scrollbar.set)\n",
    "        \n",
    "        # Configure grid weights\n",
    "        main_frame.columnconfigure(1, weight=1)\n",
    "        main_frame.rowconfigure(8, weight=1)\n",
    "        self.root.columnconfigure(0, weight=1)\n",
    "        self.root.rowconfigure(0, weight=1)\n",
    "    \n",
    "    def log_status(self, message):\n",
    "        \"\"\"Add message to status text\"\"\"\n",
    "        self.status_text.config(state=tk.NORMAL)\n",
    "        self.status_text.insert(tk.END, f\"{message}\\n\")\n",
    "        self.status_text.see(tk.END)\n",
    "        self.status_text.config(state=tk.DISABLED)\n",
    "        self.root.update_idletasks()\n",
    "    \n",
    "    def on_model_selected(self, event=None):\n",
    "        \"\"\"Enable load model button when a model is selected\"\"\"\n",
    "        if self.model_var.get() != \"Select a model...\":\n",
    "            self.load_model_btn.config(state=tk.NORMAL)\n",
    "    \n",
    "    def load_model(self):\n",
    "        \"\"\"Load the selected model\"\"\"\n",
    "        model_name = self.model_var.get()\n",
    "        if model_name == \"Select a model...\":\n",
    "            return\n",
    "            \n",
    "        self.log_status(f\"Loading {model_name} model...\")\n",
    "        \n",
    "        try:\n",
    "            if model_name == 'GAN':\n",
    "                if not os.path.exists(self.config.GAN_MODEL_PATH):\n",
    "                    raise FileNotFoundError(f\"GAN model file not found: {self.config.GAN_MODEL_PATH}\")\n",
    "                model = Generator(self.config).to(self.config.DEVICE)\n",
    "                model.load_state_dict(torch.load(self.config.GAN_MODEL_PATH, map_location=self.config.DEVICE))\n",
    "                \n",
    "            elif model_name == 'RaGAN':\n",
    "                if not os.path.exists(self.config.RAGAN_MODEL_PATH):\n",
    "                    raise FileNotFoundError(f\"RaGAN model file not found: {self.config.RAGAN_MODEL_PATH}\")\n",
    "                model = Generator(self.config).to(self.config.DEVICE)\n",
    "                model.load_state_dict(torch.load(self.config.RAGAN_MODEL_PATH, map_location=self.config.DEVICE))\n",
    "                \n",
    "            elif model_name == 'VAE':\n",
    "                if not os.path.exists(self.config.VAE_MODEL_PATH):\n",
    "                    raise FileNotFoundError(f\"VAE model file not found: {self.config.VAE_MODEL_PATH}\")\n",
    "                model = TransformerVAE(self.config).to(self.config.DEVICE)\n",
    "                model.load_state_dict(torch.load(self.config.VAE_MODEL_PATH, map_location=self.config.DEVICE))\n",
    "            \n",
    "            model.eval()\n",
    "            self.models[model_name] = model\n",
    "            self.selected_model = model_name\n",
    "            \n",
    "            self.model_status_label.config(text=f\"{model_name} model loaded successfully\", foreground=\"green\")\n",
    "            self.log_status(f\"{model_name} model loaded successfully\")\n",
    "            self.update_process_button_state()\n",
    "            \n",
    "        except Exception as e:\n",
    "            error_msg = f\"Error loading {model_name} model: {str(e)}\"\n",
    "            self.model_status_label.config(text=error_msg, foreground=\"red\")\n",
    "            self.log_status(error_msg)\n",
    "            messagebox.showerror(\"Model Loading Error\", error_msg)\n",
    "    \n",
    "    def browse_midi_file(self):\n",
    "        \"\"\"Open file dialog to select MIDI file\"\"\"\n",
    "        file_path = filedialog.askopenfilename(\n",
    "            title=\"Select MIDI File\",\n",
    "            filetypes=[(\"MIDI files\", \"*.mid *.midi\"), (\"All files\", \"*.*\")]\n",
    "        )\n",
    "        \n",
    "        if file_path:\n",
    "            self.input_midi_path = file_path\n",
    "            filename = os.path.basename(file_path)\n",
    "            self.file_label.config(text=f\"Selected: {filename}\", foreground=\"black\")\n",
    "            self.log_status(f\"Selected MIDI file: {filename}\")\n",
    "            self.update_process_button_state()\n",
    "    \n",
    "    def update_process_button_state(self):\n",
    "        \"\"\"Enable process button if both model and file are selected\"\"\"\n",
    "        if self.selected_model and self.input_midi_path:\n",
    "            self.process_btn.config(state=tk.NORMAL)\n",
    "        else:\n",
    "            self.process_btn.config(state=tk.DISABLED)\n",
    "    \n",
    "    def midi_to_matrices(self, midi_path):\n",
    "        \"\"\"Convert MIDI file to 3-channel matrix representation\"\"\"\n",
    "        try:\n",
    "            pm = pretty_midi.PrettyMIDI(midi_path)\n",
    "            \n",
    "            # Find piano tracks\n",
    "            all_piano_notes = []\n",
    "            PIANO_PROGRAMS = list(range(8))  # Piano family instruments\n",
    "            \n",
    "            for instrument in pm.instruments:\n",
    "                if instrument.program in PIANO_PROGRAMS:\n",
    "                    all_piano_notes.extend(instrument.notes)\n",
    "            \n",
    "            if not all_piano_notes:\n",
    "                raise ValueError(\"No piano tracks found in MIDI file\")\n",
    "            \n",
    "            # Sort notes by start time\n",
    "            all_piano_notes.sort(key=lambda x: x.start)\n",
    "            \n",
    "            # Calculate timing parameters\n",
    "            song_end_time = max(note.end for note in all_piano_notes)\n",
    "            resolution = 24\n",
    "            tempo_bpm = pm.get_tempo_changes()[1][0] if len(pm.get_tempo_changes()[1]) > 0 else 120.0\n",
    "            ticks_per_second = (resolution * tempo_bpm) / 60.0\n",
    "            \n",
    "            total_timesteps = int(np.ceil(song_end_time * ticks_per_second))\n",
    "            \n",
    "            # Initialize matrices\n",
    "            onset_matrix = np.zeros((total_timesteps, 88), dtype=np.float32)\n",
    "            sustain_matrix = np.zeros((total_timesteps, 88), dtype=np.float32)\n",
    "            velocity_matrix = np.zeros((total_timesteps, 88), dtype=np.float32)\n",
    "            \n",
    "            # Populate matrices\n",
    "            LOWEST_PITCH = 21\n",
    "            for note in all_piano_notes:\n",
    "                if 21 <= note.pitch <= 108:  # 88-key piano range\n",
    "                    pitch_idx = note.pitch - LOWEST_PITCH\n",
    "                    \n",
    "                    onset_step = int(round(note.start * ticks_per_second))\n",
    "                    offset_step = int(round(note.end * ticks_per_second))\n",
    "                    \n",
    "                    if onset_step < total_timesteps:\n",
    "                        # Onset and velocity\n",
    "                        onset_matrix[onset_step, pitch_idx] = 1.0\n",
    "                        velocity_matrix[onset_step, pitch_idx] = note.velocity / 127.0\n",
    "                        \n",
    "                        # Sustain\n",
    "                        sustain_end = min(offset_step, total_timesteps)\n",
    "                        sustain_matrix[onset_step:sustain_end, pitch_idx] = 1.0\n",
    "            \n",
    "            return onset_matrix, sustain_matrix, velocity_matrix\n",
    "            \n",
    "        except Exception as e:\n",
    "            raise Exception(f\"Error converting MIDI to matrices: {str(e)}\")\n",
    "    \n",
    "    def matrices_to_segments(self, onset_matrix, sustain_matrix, velocity_matrix):\n",
    "        \"\"\"Convert matrices to model input segments\"\"\"\n",
    "        segments = []\n",
    "        timesteps = self.config.TIMESTEPS\n",
    "        \n",
    "        # Combine matrices into 3-channel representation\n",
    "        combined_matrix = np.stack([onset_matrix, sustain_matrix, velocity_matrix], axis=0)  # Shape: (3, time, pitch)\n",
    "        \n",
    "        # Split into segments\n",
    "        total_time = combined_matrix.shape[1]\n",
    "        num_segments = (total_time + timesteps - 1) // timesteps  # Ceiling division\n",
    "        \n",
    "        for i in range(num_segments):\n",
    "            start_idx = i * timesteps\n",
    "            end_idx = min(start_idx + timesteps, total_time)\n",
    "            \n",
    "            # Extract segment\n",
    "            segment = combined_matrix[:, start_idx:end_idx, :]\n",
    "            \n",
    "            # Pad if necessary\n",
    "            if segment.shape[1] < timesteps:\n",
    "                padding = np.zeros((3, timesteps - segment.shape[1], 88))\n",
    "                segment = np.concatenate([segment, padding], axis=1)\n",
    "            \n",
    "            segments.append(torch.tensor(segment, dtype=torch.float32))\n",
    "        \n",
    "        return segments\n",
    "    \n",
    "    def segments_to_midi(self, segments, output_path, onset_threshold=0.5):\n",
    "        \"\"\"Convert model output segments back to MIDI\"\"\"\n",
    "        # Concatenate all segments\n",
    "        full_tensor = torch.cat(segments, dim=1)  # Shape: (3, total_time, 88)\n",
    "        \n",
    "        pm = pretty_midi.PrettyMIDI(initial_tempo=120.0)\n",
    "        instrument = pretty_midi.Instrument(program=0, name='Acoustic Grand Piano')\n",
    "        \n",
    "        onset_probs = full_tensor[0].numpy()\n",
    "        sustain_probs = full_tensor[1].numpy()\n",
    "        velocity_values = full_tensor[2].numpy()\n",
    "        \n",
    "        reconstructed_onset = (onset_probs > onset_threshold).astype(int)\n",
    "        \n",
    "        resolution = 24\n",
    "        lowest_pitch = 21\n",
    "        tick_duration = 60.0 / (120.0 * resolution)\n",
    "        \n",
    "        active_notes = {}\n",
    "        total_timesteps = reconstructed_onset.shape[0]\n",
    "        \n",
    "        for t_step in range(total_timesteps):\n",
    "            for pitch_idx in range(88):\n",
    "                if pitch_idx not in active_notes and reconstructed_onset[t_step, pitch_idx] == 1:\n",
    "                    start_time = t_step * tick_duration\n",
    "                    velocity = int(velocity_values[t_step, pitch_idx] * 126) + 1\n",
    "                    active_notes[pitch_idx] = (start_time, velocity)\n",
    "                elif pitch_idx in active_notes and sustain_probs[t_step, pitch_idx] < 0.5:\n",
    "                    start_time, velocity = active_notes.pop(pitch_idx)\n",
    "                    end_time = t_step * tick_duration\n",
    "                    if end_time > start_time:\n",
    "                        instrument.notes.append(pretty_midi.Note(\n",
    "                            velocity=max(1, min(127, velocity)),\n",
    "                            pitch=pitch_idx + lowest_pitch,\n",
    "                            start=start_time,\n",
    "                            end=end_time\n",
    "                        ))\n",
    "        \n",
    "        # Close any remaining active notes\n",
    "        for pitch_idx, (start_time, velocity) in active_notes.items():\n",
    "            end_time = total_timesteps * tick_duration\n",
    "            if end_time > start_time:\n",
    "                instrument.notes.append(pretty_midi.Note(\n",
    "                    velocity=max(1, min(127, velocity)),\n",
    "                    pitch=pitch_idx + lowest_pitch,\n",
    "                    start=start_time,\n",
    "                    end=end_time\n",
    "                ))\n",
    "        \n",
    "        pm.instruments.append(instrument)\n",
    "        pm.write(output_path)\n",
    "    \n",
    "    def midi_to_wav(self, midi_path, wav_path):\n",
    "        \"\"\"Convert MIDI to WAV using FluidSynth\"\"\"\n",
    "        try:\n",
    "            if not os.path.exists(self.config.SOUNDFONT_PATH):\n",
    "                raise FileNotFoundError(f\"Soundfont not found: {self.config.SOUNDFONT_PATH}\")\n",
    "            \n",
    "            pm = pretty_midi.PrettyMIDI(midi_path)\n",
    "            audio_data = pm.fluidsynth(fs=44100)\n",
    "            \n",
    "            with wave.open(wav_path, 'wb') as wf:\n",
    "                wf.setnchannels(1)\n",
    "                wf.setsampwidth(2)\n",
    "                wf.setframerate(44100)\n",
    "                wf.writeframes((audio_data * 32767).astype(np.int16).tobytes())\n",
    "                \n",
    "        except Exception as e:\n",
    "            raise Exception(f\"Error converting MIDI to WAV: {str(e)}\")\n",
    "    \n",
    "    def process_midi_thread(self):\n",
    "        \"\"\"Process MIDI file in separate thread\"\"\"\n",
    "        try:\n",
    "            self.log_status(\"Starting MIDI style transfer process...\")\n",
    "            \n",
    "            # Create output directory\n",
    "            os.makedirs(self.config.OUTPUT_DIR, exist_ok=True)\n",
    "            \n",
    "            # Step 1: Convert MIDI to matrices\n",
    "            self.log_status(\"Converting MIDI to matrices...\")\n",
    "            onset_matrix, sustain_matrix, velocity_matrix = self.midi_to_matrices(self.input_midi_path)\n",
    "            self.log_status(f\"Matrix shapes: {onset_matrix.shape}\")\n",
    "            \n",
    "            # Step 2: Convert to segments\n",
    "            self.log_status(\"Preparing model input segments...\")\n",
    "            input_segments = self.matrices_to_segments(onset_matrix, sustain_matrix, velocity_matrix)\n",
    "            self.log_status(f\"Created {len(input_segments)} segments\")\n",
    "            \n",
    "            # Step 3: Run model inference\n",
    "            self.log_status(f\"Running {self.selected_model} model inference...\")\n",
    "            model = self.models[self.selected_model]\n",
    "            output_segments = []\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for i, segment in enumerate(input_segments):\n",
    "                    input_tensor = segment.unsqueeze(0).to(self.config.DEVICE)  # Add batch dimension\n",
    "                    \n",
    "                    if self.selected_model in ['GAN', 'RaGAN']:\n",
    "                        noise = torch.randn(1, self.config.NOISE_DIM).to(self.config.DEVICE)\n",
    "                        output = model(input_tensor, noise)\n",
    "                    elif self.selected_model == 'VAE':\n",
    "                        output, _, _ = model(input_tensor)\n",
    "                    \n",
    "                    output_segments.append(output.squeeze(0).cpu())  # Remove batch dimension\n",
    "                    \n",
    "                    if (i + 1) % 10 == 0:\n",
    "                        self.log_status(f\"Processed {i + 1}/{len(input_segments)} segments\")\n",
    "            \n",
    "            # Step 4: Convert back to MIDI\n",
    "            base_filename = os.path.splitext(os.path.basename(self.input_midi_path))[0]\n",
    "            midi_output_path = os.path.join(self.config.OUTPUT_DIR, f\"{base_filename}_{self.selected_model}_output.mid\")\n",
    "            wav_output_path = os.path.join(self.config.OUTPUT_DIR, f\"{base_filename}_{self.selected_model}_output.wav\")\n",
    "            \n",
    "            self.log_status(\"Converting output to MIDI...\")\n",
    "            onset_threshold = 0.1 if self.selected_model == 'VAE' else 0.5\n",
    "            self.segments_to_midi(output_segments, midi_output_path, onset_threshold)\n",
    "            \n",
    "            # Step 5: Convert MIDI to WAV\n",
    "            self.log_status(\"Converting MIDI to WAV...\")\n",
    "            self.midi_to_wav(midi_output_path, wav_output_path)\n",
    "            \n",
    "            self.log_status(f\"✓ Process completed successfully!\")\n",
    "            self.log_status(f\"✓ MIDI saved to: {midi_output_path}\")\n",
    "            self.log_status(f\"✓ WAV saved to: {wav_output_path}\")\n",
    "            \n",
    "            # Show completion message\n",
    "            self.root.after(0, lambda: messagebox.showinfo(\n",
    "                \"Success\", \n",
    "                f\"Style transfer completed!\\n\\nFiles saved to:\\n{self.config.OUTPUT_DIR}\"\n",
    "            ))\n",
    "            \n",
    "        except Exception as e:\n",
    "            error_msg = f\"Error during processing: {str(e)}\"\n",
    "            self.log_status(f\"✗ {error_msg}\")\n",
    "            self.root.after(0, lambda: messagebox.showerror(\"Processing Error\", error_msg))\n",
    "        \n",
    "        finally:\n",
    "            # Re-enable button and stop progress bar\n",
    "            self.root.after(0, self.processing_complete)\n",
    "    \n",
    "    def process_midi(self):\n",
    "        \"\"\"Start MIDI processing in separate thread\"\"\"\n",
    "        self.process_btn.config(state=tk.DISABLED)\n",
    "        self.progress.start()\n",
    "        \n",
    "        # Start processing in separate thread\n",
    "        thread = threading.Thread(target=self.process_midi_thread)\n",
    "        thread.daemon = True\n",
    "        thread.start()\n",
    "    \n",
    "    def processing_complete(self):\n",
    "        \"\"\"Called when processing is complete\"\"\"\n",
    "        self.progress.stop()\n",
    "        self.process_btn.config(state=tk.NORMAL)\n",
    "    \n",
    "    def run(self):\n",
    "        \"\"\"Start the GUI application\"\"\"\n",
    "        self.log_status(\"MIDI Style Transfer Application Started\")\n",
    "        self.log_status(\"1. Select a model and click 'Load Model'\")\n",
    "        self.log_status(\"2. Browse and select a MIDI file\")\n",
    "        self.log_status(\"3. Click 'Generate Style Transfer' to process\")\n",
    "        \n",
    "        self.root.mainloop()\n",
    "\n",
    "# Create and run the application\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        app = MidiStyleTransferApp()\n",
    "        app.run()\n",
    "    except Exception as e:\n",
    "        print(f\"Error starting application: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "else:\n",
    "    # For Jupyter notebook\n",
    "    app = MidiStyleTransferApp()\n",
    "    app.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45864ba9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fyp_env_2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
